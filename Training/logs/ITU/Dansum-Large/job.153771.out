Running on desktop17:
Loading anaconda
Sourcing .bashrc
stdin: is not a tty
Activating virtual environment
Logging in to huggingface
Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.
Token is valid (permission: write).
Your token has been saved to /home/easc/.cache/huggingface/token
Login successful
Running script
[nltk_data] Downloading package punkt to /home/easc/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
/home/easc/.conda/envs/env_easc/lib/python3.9/site-packages/pyarrow/pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.
  if _pandas_api.is_sparse(col):
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
start
done

          Running main.py with the following parameters: 
          max_input_length: 3500 
          max_target_length: 500 
          batch_size: 1 
          num_epochs: 10 
          num_steps: 4
          model_checkpoint: Danish-summarisation/DanSumT5-large
          data_source: Resumes_without_doubles_Final.csv
          
4.32.1
tokenized_datasets
Map:   0%|          | 0/800 [00:00<?, ? examples/s]Map: 100%|██████████| 800/800 [00:04<00:00, 162.01 examples/s]                                                              Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 201.13 examples/s]                                                              Map:   0%|          | 0/100 [00:00<?, ? examples/s]Map: 100%|██████████| 100/100 [00:00<00:00, 169.47 examples/s]                                                              Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
model_checkpoint
cuda_memory
Seq2SeqTrainingArguments
data_collator
benchmark = false
empty cache
start training
  0%|          | 0/2000 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/easc/main.py", line 226, in <module>
    main()
  File "/home/easc/main.py", line 220, in main
    trainer.train()
  File "/home/easc/.conda/envs/env_easc/lib/python3.9/site-packages/transformers/trainer.py", line 1546, in train
    return inner_training_loop(
  File "/home/easc/.conda/envs/env_easc/lib/python3.9/site-packages/transformers/trainer.py", line 1837, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/easc/.conda/envs/env_easc/lib/python3.9/site-packages/transformers/trainer.py", line 2682, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/easc/.conda/envs/env_easc/lib/python3.9/site-packages/transformers/trainer.py", line 2707, in compute_loss
    outputs = model(**inputs)
  File "/home/easc/.conda/envs/env_easc/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/easc/.conda/envs/env_easc/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/easc/.conda/envs/env_easc/lib/python3.9/site-packages/accelerate/utils/operations.py", line 659, in forward
    return model_forward(*args, **kwargs)
  File "/home/easc/.conda/envs/env_easc/lib/python3.9/site-packages/accelerate/utils/operations.py", line 647, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/home/easc/.conda/envs/env_easc/lib/python3.9/site-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast
    return func(*args, **kwargs)
  File "/home/easc/.conda/envs/env_easc/lib/python3.9/site-packages/transformers/models/mt5/modeling_mt5.py", line 1734, in forward
    encoder_outputs = self.encoder(
  File "/home/easc/.conda/envs/env_easc/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/easc/.conda/envs/env_easc/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/easc/.conda/envs/env_easc/lib/python3.9/site-packages/transformers/models/mt5/modeling_mt5.py", line 1096, in forward
    layer_outputs = layer_module(
  File "/home/easc/.conda/envs/env_easc/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/easc/.conda/envs/env_easc/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/easc/.conda/envs/env_easc/lib/python3.9/site-packages/transformers/models/mt5/modeling_mt5.py", line 559, in forward
    self_attention_outputs = self.layer[0](
  File "/home/easc/.conda/envs/env_easc/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/easc/.conda/envs/env_easc/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/easc/.conda/envs/env_easc/lib/python3.9/site-packages/transformers/models/mt5/modeling_mt5.py", line 464, in forward
    attention_output = self.SelfAttention(
  File "/home/easc/.conda/envs/env_easc/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/easc/.conda/envs/env_easc/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/easc/.conda/envs/env_easc/lib/python3.9/site-packages/transformers/models/mt5/modeling_mt5.py", line 393, in forward
    scores = torch.matmul(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 374.00 MiB. GPU 0 has a total capacty of 47.45 GiB of which 9.58 GiB is free. Including non-PyTorch memory, this process has 37.88 GiB memory in use. Of the allocated memory 37.24 GiB is allocated by PyTorch, and 450.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
  0%|          | 0/2000 [00:05<?, ?it/s]
Done
